{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another source of error = OVERFITTING\n",
    "* Overfitting = when model is so excessively complex that it starts to catch random noise instead of describing the true underlying relationship\n",
    "* typically manifested with model that evaluates as more accurate than it really is. \n",
    "* in most situations you shouldn't be able to build a perfect model--> some error is to be expected. \n",
    "* it is extremely common and easy to do \n",
    "\n",
    "Weve been using same data to train the model and see how well model is doing. \n",
    "* some danger of this approach can be apparent. \n",
    "* if we create a very elaborate model, it will pick up on the nuances of the data that are just from random noise. \n",
    "* if we evaluate the model on the training data, that ability to pick up noise will be returned as accuracy \n",
    "* in reality, this isnt the case and doesnt depict how wed really want to evaluate a model\n",
    "* we generally dont care about predicting things we already know. \n",
    "* we care about other data, new info, or other situations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Groups\n",
    "\n",
    "Simplest way to combat overfitting is with --> Holdout group (holdback group)\n",
    "* You do not include all of your data in your trianing set, and instead reserve some of it exclusively for testing. \n",
    "* there is a cost to having less training data, but evaluation will be more reliable\n",
    "\n",
    "when directly comparing two models that are based on different techniques or different specifications, this holdout method combats overfitting. \n",
    "* overfit models will see a drop in success rate outside of their training data, and os their performance will not be artificially inflated as it would be if you trained and validated your model using the whold data set. \n",
    "* this is b/c they got really good at matching the patterns within the data they were trained with but didnt actually learn the things that matter but random noise. \n",
    "* when they try to match that random noise on new data their accuracy suffers. \n",
    "\n",
    "how much data you choose to keep in a holdout is really up to you and depends on how much and what kind of data you have to begin with as well as what kind of model youre training with . \n",
    "* should chekc and see how much varianve your model has as you add more data as well as how much data it would take to maintain a reasonably representative test sample. \n",
    "* 30% is a common starting point but really anything from 50% to 1% of original dataset is reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.884304932735426\n",
      "Testing on Sample: 0.8916008614501076\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "The more robust version of holdout groups--> cross validation\n",
    "* instead of creating just one holdout, you create several\n",
    "\n",
    "The way it works: \n",
    "* break up data into several equally sized pieces (folds)\n",
    "    * lets say you make x folds. \n",
    "* go through the training and testing process x times, each time with a different fold held out from the training data and used as the test set. \n",
    "* number of folds you create is up to you but will depend on how much data you want in your testing set. \n",
    "* This kind of cross validation is called--> Leave one out\n",
    "    * useful if youre worried about single obs skewing your model. whereas large folds combat more general overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89784946, 0.89426523, 0.89426523, 0.890681  , 0.89605735,\n",
       "       0.89048474, 0.88150808, 0.89028777, 0.88489209, 0.89568345])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the array that cross_val_score returns = series of accuracy scores with a different hold out group each time. if our model is overfitting at a variable amount, those scores will fluctuate. \n",
    "* the one above is realtively constant. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you understand how cross validation works, try to code it up yourself below, not relying on SKLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-baf16bdde77f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-baf16bdde77f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    1.) test\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Implement your own cross validation with your spam model.\n",
    "1.) test\n",
    "    cross_val_1 = 0.20 * data w. random number generated seed 1\n",
    "    cross_val_2 = 0.20 * data w. random number generated seed 2\n",
    "    cross_val_3 = 0.20 * data w. random number generated seed 3\n",
    "    cross_val_4 = 0.20 * target w. random number genereated seed 4\n",
    "    cross_val_5 = 0.20 * target w. random number generated seed 5\n",
    "    cross_val_6 = 0.20 * target w. random numbe generated seed 6\n",
    "    \n",
    "    train\n",
    "    cross_val_1_t = 0.80 * data w. random number generated seed 1\n",
    "    cross_val_2_t = 0.80 * data w. random number generated seed 2\n",
    "    cross_val_3_t = 0.80 * data w. random number generated seed 3\n",
    "    cross_val_4_t = 0.80 * target w. random number genereated seed 4\n",
    "    cross_val_5_t = 0.80 * target w. random number generated seed 5\n",
    "    cross_val_6_t = 0.80 * target w. random numbe generated seed 6\n",
    "    \n",
    "2.) \n",
    "    train\n",
    "    model_1 = nbn(cross_val_1_t = data * cross_val_4_t = target)\n",
    "    model_2 = nbn(cross_val_2_t = data * cross_val_5_t = target)\n",
    "    model_3 = nbn(cross_val_3_t = data * cross_val_6_t = target)\n",
    "    \n",
    "    test\n",
    "    model_1.score(cross_val_1, cross_val_4)\n",
    "    model_2.score(cross_val_2, cross_val_5)\n",
    "    model_3.score(cross_val_3, cross_val_6)\n",
    "    \n",
    "3.) score = [0.88, 0.87, 0.86]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a good score?\n",
    "When look at model, getting accuracy scores around .89\n",
    "* seems like a pretty good score, but we there are different kind of errors\n",
    "* Class imbalance\n",
    "* Both are played here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THinking like a data scientist\n",
    "how you choose to balidate model will depend on kind of data and concerns\n",
    "* model is trained to fit the data you feed it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Naive Bayes\n",
    "Niave bayes is good for avoiding overfitting\n",
    "* B/c assumptions are so simple\n",
    "* particualarly the assumed independence b/w any two ind variables\n",
    "* one sources of overfitting is when model tries to map complex interactions b/w variables that arent really there or significant\n",
    "* naive bayes cant do this b/c it assumes they are all ind and not interacting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
